---
title: "Multivariate Analysis Workshop"
author: "R. Alex Thompson & Madeline Franz"
date: "4/11/2022"
output: pdf_document
---

Welcome to the Multivariate Statistics in Ecology and Earth Sciences Workshop 2022. The central aims of this workshop are: (i) introduce attendees to the basic principles behind various multivariate methods, (ii) outline a general strategy for deciding if a multivariate approach is best for your data, and (iii) work through examples of multivariate analysis in R, providing sample code and data for future exploration and learning.  
  
__Overview of Multivariate Analysis and Multivariate Data__  
Technological advances have made it easier for researchers to measure many more variables at once than has been historically possible. Multivariate data lets us view a study system from a variety of perspectives (re: dimensions) and has helped us to learn a great deal about natural systems in the process. Yet, without multivariate analysis, it would be difficult to make sense of all of how these different puzzle pieces all fit together. Examples of this include coordinated trait responses, variable interactions, contrasts, and latent effects. Put simply, multivariate analysis can be thought of as a tool that allows us to examine all of the puzzle pieces at once to find those which fit "best" among all possible combinations.

__Why Multivariate Analysis__  
So, why multivariate analysis? The archetypal scientist might just as well argue for evaluating correlations among variables one-by-one. This technique might work for some variables (i.e. photoperiod and plant growth) but not so well for other variables that are clearly coupled (i.e. growth and photosynthesis).  In the latter example, latent, hidden effects that we can't (or simply don't) measure or account for are responsible for driving relationships. This is a classic example of Gestalt: the whole is greater than the sum of its parts. Multivariate analysis is like a peak behind the curtain, letting us look at how different combinations or groups of variables affect another variable. More importantly, multivariate analysis can let us look at how multiple variables affect multiple other variables.  
  
Another example is how observations might group themselves. As humans, we like to categorize or cluster things we see in nature. Species, clouds, rocks, etc. While these categories can be useful for making sense of the world, it is often the case that nature doesn't care about our tidy categories. More often than not, we are interested in the cases where nature breaks our rules. How then, can we group phenomena more heuristically? The answer is found in multivariate analysis and we specifically address these examples in this tutorial. To understand how we can do this, the next section sets the stage for a geometric understanding of multivariate analysis.  
  
  
__When To Use Multivariate Analysis (and When Not To)__  
While multivariate analysis can be an incredibly powerful tool, it can also be misused. How do we exploit the power of multivariate statistics, while avoiding some of these misuses? The first step is to evaluate whether or not your data is a good fit for the technique at hand. Once you think you have data that might benefit from a multivariate analysis, use the table below to check which of the methods we walk through in this course are a good candidate. Most modern statistical methods use multivariate perspectives and this is in no way a comprehensive guide but rather a "How to" for getting started.  

  
__How to use Multivariate Analysis__  
In this tutorial, we will introduce you to four methods used in multivariate statistics: Canonical Correlation, Clustering Algorithms, Classification, and Ordination (Principal Components Analysis). In order to illustrate these techniques in the simplest way, we have chosen to use the infamous _iris_ dataset provided in RStudio. This dataset, while simple, is very well-behaved. While we seldom have well-behaved data in practice, this makes teaching multivariate methods much easier. Not only will the basic principles of multivariate analysis become more clear, but our ability to make conclusions will be simplified. This is in line with aim (i), and is the motivating aim of this tutorial.    
  
Let us start by first exploring the _iris_ dataset, so that we may develop a more intuitive sense of the project at hand.

```{r open dataset}
library(tidyverse) # a majority of the code in this tutorial will be based in the tidyverse
library(flextable) # this is a good package for making nice tables but feel free to use whatever you like
iris<-iris # load the data that comes with R
iris # view the data
```
  
Now, if we inspect our data we see that it contains 150 observations over just 5 variables, one of them being categorical. This is great, because when $n \gt\gt p$, computation is greatly simplified.  
  
Let us first consider the scenario where we collect this data with the goal of interrogating relationships between the variables, grouping of variables, and the species. There's a lot to unpack here. First, we could simply run linear (or nonlinear) regression models on each pair of variables separately, giving six unique figures and statistical outputs. That's not to mention if we decide to group by species, which further complicates things. In reality, most of our data contains variables that are correlated. A motivating example is climate data, where temperature, precipitation, and VPD can all interact. Clearly, this is not preferable since correlations between variables can lead to unnecessarily large p, make prediction more difficult, and may lead to overfitting.  
  
Let us check for correlations in our data.  
  
```{r check for correlations}
cor(iris[1:4])
```
  
It appears that most of our variables are correlated, which limits what we may include in a regression model. However, what if we want to test how a set of multiple variables correlates with another set of multiple variables? In mathematical terms, we split our data into two (not necessarily equally sized!) matrices: $\textbf{p}_1$ and $\textbf{p}_2$. Although the number of variables in each of the $\textbf{p}_i$ matrices can be unequal, the number of observations must be equal for the analysis to work.  
  
We are now ready to introduce our first technique, _Canonical Correlation_. INSERT INFO ABOUT CANONICAL CORR.
  
Before we can do anything, however, it's best to scale and center our data. What does this mean? In short, if some of our variables have more variance than others, or are measured at a different scale (i.e. values are in the 100s versus 10s), this can create issues with multivariate techniques. A fundamental component of any multivariate analysis is the concept of distance. If one variable has greater variance than the other variables, it is likely that your results will be strongly influenced by this single variable. Obviously, this is bad. To correct for this we can use R's built in scale and center functions to give our data unit variance - essentially homogenizing the variance of each column so that they are equally weighted in the calculations.  
  
``` {r scale and center}
iris_scale<-scale(iris[1:4])
```
  
``` {r canonical correlation}
#Install packages
library(CCA) #for canonical correlations
library(CCP) #to assess statistical significance of canonical correlations

#Canonical Correlation
##Select the variables to be used in each of the two categories
sepal <- iris[,c(1:2)]
petal <-iris[,c(3:4)]

matcor(sepal, petal) #displays correlations within and between categories
iris_cc <- cc(sepal, petal)#this function performs the canonical correlation analysis

# display the canonical correlations
iris_cc$cor

# raw canonical coefficients
iris_cc[3:4]

# compute canonical loadings
iris_cc2 <- comput(sepal, petal, iris_cc)

# display canonical loadings
iris_cc2[3:6]

rho <- iris_cc$cor #make a vector of the canonical correlation coefficients

n <- dim(sepal)[1] #number of observations of each variable
p <- length(sepal) #number of 'independent' variables
q <- length(petal) #number of 'dependent' variables

p.asym(rho, n, p, q, tstat = 'Wilks') #specify test statistic to be used. could also use Hotelling, Pillai or Roy

```
  
Often, we want to group our data. While grouping can be done a priori during the data collection process (i.e. treatments, sites, etc.), sometimes we don't know what the groups are. There are two general approaches associated with post hoc grouping: _Clustering_ and _Classification_.  
  
Let's start with Classification. INSERT INFO ABOUT CLASSIFICATION.  
  
``` {r classification}
library(MASS)
library(ggplot2)

#make this example reproducible
set.seed(666)

#Here we split the data into training and testing sets. We use 70% of dataset as training set (randomly selected) and the remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.7,0.3)) #select 70% of the rows/observations
train <- iris[sample, ] #the selected 70% become the training data
test <- iris[!sample, ] #everything not selected (30%) becomes testing data

#fit the linear discriminant analysis model on the training data
iris_lda <- lda(Species~., data=train)

#view model output
iris_lda

#use LDA model to make predictions on test data
predicted <- predict(iris_lda, test)#here the predictions are made

mean(predicted$class==test$Species)#here we test the predictions by comparing them to the known species to determine if the predictions are correct
missclass <- predicted$class==test$Species #here we can see which predictions are missclassified (show up as 'FALSE')

#define data to plot
lda_train_plot <- cbind(train, predict(iris_lda)$x) #adds 2 columns of discriminants to the training points
lda_test_plot <- cbind(test, predicted$x) #adds 2 columns of discriminants to the testing points

miss_dat <- cbind(lda_test_plot, missclass) #adds the 'misclassification' true/false to the testing points
lda_miss_plot <- miss_dat[missclass==FALSE,] #selects only the rows that were missclassified

#create plot
ggplot(lda_train_plot, aes(LD1, LD2)) + #initiates the plot and sets axes based on the linear discriminats
  geom_point(aes(color = Species)) + #adds the training points to the graph, colored by their species
    geom_point(data = lda_test_plot, aes(color = Species)) + #adds testing points to the graph, colored by their actual species
      geom_point(data = lda_miss_plot, shape=4) #puts an 'x' over the misclassified points
```
  
Clearly, _Classification_ requires some a priori knowledge about our data. Furthermore, it's strength comes in classifying _new_ observations. But what if we don't have new observations and we just want to group our existing observations? Well, this is where _Clustering_ algorithms can help. Clustering algorithms are highly effective mathematical tools that leverage concepts of distance between points in multiple dimensions ($p\ge1$) to find the best groups. Sometimes our data makes this easy. Sometimes it is not.  
  
__K-means__
There are many different algorithms that can be used in clustering. These include hierarchical methods like single linkage or average linkage. What we're going to cover here is the k-means algorithm which is widely used across many fields. The basic idea is to imagine a moving window across all of your data points. This window is calculating the distance between points and clustering them based on minimum distances. Essentially, you specify k clusters (k = 1, 2, 3, etc.) and run the algorithm. The distance of each observation is compared the average distance of each of the k clusters. The points in each cluster are shuffled based on this algorithm until the within-cluster distance is minimized while the between-cluster distance is maximized. Intuitively this makes sense - we group close observations together and maximize the distance between the (fewest number of) groups.  
  
_K-means algorithm_  
_1. Specify the number of K clusters_  
_2. Randomly shuffle K observations into K clusters to find the initial centroids_  
_3. For every observation, compute the sum of squared distances from initial centroids and assign to closest cluster_  
_4  Repeat 2 & 3 until within cluster sum of squared distances is minimized, and between cluster sum of squared distances is maximized_  
  
Let us take the _iris_ data make an attempt at clustering it.
  
  
``` {r clustering}
k2<-kmeans(iris[1:4],centers=2) # run the kmeans algorithm included in R
# first we specify the data where the [1:4] command specifies which columns to evaluate
# the centers command is where we specify the number of clusters 
k2 # let's look at our clusters
# just two clusters capture 77% of the variance in the data. That's pretty good!
k3<-kmeans(iris[1:4],centers=3) # let's try 3 clusters to see if we can improve our results
k3 # looking at our results 
# we get marginal improvement, increasing the variance explained by two percentage points
# given the tradeoff, it seem slike k=2 clusters is probably best
# recall: we want to maximize variance explained in the fewest number of clusters

# now we want to update our original dataset to reflect these clusters
group<-k2$cluster # let's isolate the vector that defines which observations belong in which cluster
iris2<-cbind(iris,group) # combine the original data with the cluster vector
iris2$group<-as.factor(iris2$group)

# now we want to explore things a big more. 
# we know that there are 3 groups in our data: the 3 species
# Do we have patterns in how our clusters are grouped? 
# let's make a figure to visualize this clustering
library(ggplot2) # we'll use ggplot to make figures
group_fig<-ggplot(iris2,aes(Species,fill=group)) + geom_bar()
# we can see pretty clearly now that all of the setosa and virginica species are classified into separate groups
# it looks like versicolor, however, is split between clusters
# this warrants further investigation
# let's extract our cluster centers for each variable, and inspect what makes our clusters different
centers<-k2$centers # extract the centers
centers # view the centers
# group 1 can be described as having longer sepals, and overall larger petals
# group 2 has shorter sepals that are more wide, with much smaller petals than group 2
# so, perhaps our versicolor species that were classified as group 2 were simply outliers
# let's move on to PCA, and see how we can apply these clusters in a more interesting way that let's us compare them in more detail. 
```
  
The essential goal of most multivariate methods is to try and reduce the dimensionality of our data in a manner that let's us make sense of it. As humans, we cannot even visualize dimensions greater than 3, let alone explain it. We've developed incredible tools that leverage theorems from linear algebra to reduce matrices of multiple dimensions into a smaller subset of dimensions that retains the original variability of the data. What does this mean? Let's go back to linear regression.  
  
In their simplest form, linear regression models predict a y variable from an x variable, often coming in the form of $y=\beta_0+x\beta_1+\epsilon$. For most research objectives, this form (and its derivatives) seems to work just fine. In the multivariate case, however, we often want to explore several variables at once. We can use a technique called _Principal Components Analysis_ to reduce a set of $p$ variables into a minimal set of variables through a process called _eigenanalysis_. Let's briefly discuss this process so you have a better idea of how PCA works, and why it's effective.  
  
__Eigenanalysis: The Basics__  
Consider a dataset you've collected at one of your study sites. Let this dataset have $p=4$ random variables, $v_1$, $v_2$, $v_3$, and $v_4$. We can express our dataset, $\bf{X}$, in the form $\bf{X} = c_1v_1+c_2v_2+c_3v_3+c_4v_4$. Clearly, this presents issues if we want to visualize and analyze our data. In the sense of a scatterplot, each variable represents a new dimension and now we have 4 dimensions. Eigenanalysis allows us to scale this 4-D coordinate system in 1, 2, or 3 dimensions. Let's break down some of the very basic elements of this process a bit further.  
  
\[
\begin{matrix}
\bf{X} = c_1v_1+c_2v_2+c_3v_3+c_4v_4\\
We~want~to~express~our~data~in~linear~form~such~that~it~can~be~represented~as:\\
Y=X\beta\\
Essentially,~\beta~is~a~scaling~feature~that~takes~linear~combinations~of~all~of~our~variables\\
and~makes~new~variables:~PC1,~PC2,~etc.,~yielding~the~form:\\
Y_1=e_1X_1+\cdot\cdot\cdot+e_pX_p\\
\cdot\\
\cdot\\
\cdot\\\
Y_p=e_1X_1+\cdot\cdot\cdot+e_pX_p\\
\end{matrix}
\]  
  
We are not going to concern ourselves with how to calculate eigenvalues, as we can do this easily in R. _(Note: for those interested, this stackexchange comment (https://stats.stackexchange.com/questions/220243/the-proof-of-shrinking-coefficients-using-ridge-regression-through-spectral-dec) gives a really great and intuitive explanation of eigenvalues using spectral decomposition. I find the explanation pretty intuitive and informative, and hopefully it helps you too.)_ Essentially each $Y_i$ is an _eigenvalue_ (AKA Principal Component) and each $e_i$ is an _eigenvector_ with p rows corresponding to p variables (essentially they are scale factors). Each $Y_i$ (formally presented as $\lambda_i$) is essentially a projection of all of the variables, scaled by their corresponding eigenvectors in a 1-dimensional subspace spanning the column space of the initial data matrix. This projection is what we see when we visualize a PCA using a biplot.  
  
Some notes on PCA:  
  
1. PCA is often referred to as a dimension reduction technique, though it is not a true method of dimension reduction. In reality, PCA is simply a projection method that attempts to summarize a large number of parameters (variables) into fewer dimensions.  
  
2. PCA should typically be done on standardized and centered variables (i.e. a correlation matrix). Often, variables in a dataset can vary in scale. This can overwhelm the PCA method and create situations where a single variable is driving the summarization. Obviously this is problematic and the sensitivity of this method to such problems warrants the use of correlation matrices in PCA.  
  
3. Eigenvalues and eigenvectors are not unique to PCA. Such values are used in a variety of other statistical approaches not covered here. Having a good sense of the role of eigenvalues and what they are will prepare you for a future that likely includes encountering them again.  
  
4. R has two functions for PCA: prcomp() and princomp(). Each method employs different approaches for finding the PCAs but the function prcomp() is typically the preferred method due to greater accuracy. However, in practice, you shouldn't really notice any difference.  
  
Let's dive in.  
  
  
First, we'll start with a very simple PCA using the Iris dataset.  
  
``` {r PCA iris}
library(tidyverse)
library(ggbiplot)


# run a PCA on the iris data 
# we set center = TRUE and scale = TRUE to make sure the variance of one variable doesn't overpower the analysis and give weird results
iris_pca<-prcomp(iris[1:4],center=TRUE,scale=TRUE)
# let's look at our results
summary(iris_pca)
# it looks like 95% of the variance in the data can be explained with just two components!
iris_pca # This line of code let's us look into how each variable changes along each PC


# Now, let's plot it and see what it looks like
ggbiplot(iris_pca,choices=1:2,scale=0.05)

# we can also color the points by species to see if our groups are clustering by species
ggbiplot(iris_pca,choices=1:2,scale=0.05,groups=iris$Species)

# It looks like species are definitely breaking out along PC1
# Remember our k-means clusters above? Let's bring that data in and see if our clusters do the same thing
ggbiplot(iris_pca,choices=1:2,scale=0.05,groups=iris2$group)
```
  
Now, let's use some more complicated data and show how things can get weird and beautiful.  
  
``` {r PCA}
library(tidyverse)
library(ggbiplot)

# let's start by creating a new working directory wherever you have your data stored
setwd("") # insert the name of your directory into the quotations and run this line

# let's load in all of the data
pd<-read.csv("masterpd.csv")%>%
  dplyr::mutate(psipd=abs(psipd))%>%
  dplyr::mutate(psimd=abs(psimd))
leaf<-read.csv("leaftraits.csv")
elements<-read.csv("CN.csv")

# run the PCA on just the water potential data
pd_pca<-prcomp(pd[3:6],center=TRUE,scale.=TRUE) # runs the PCA on selected columns (MUST BE NUMERIC)
# center and scale terms must be true in order to account for different data types
pd_summary<-as.data.frame(summary(pd_pca)$importance) 
group<-pd$Group
pd_plot<-ggbiplot(pd_pca,ellipse=TRUE,groups=group)

# run PCA with combined leaf trait and water potential data
lpd<-merge(leaf,pd)%>%
  select(-date,-psidif)
lpd_pca<-prcomp(lpd[8:13],center=TRUE,scale.=TRUE)
lpd_summary<-as.data.frame(summary(lpd_pca)$importance) # 65% of the data with 2, 81% of the data with 3
group<-lpd$Group
lpd_plot<-ggbiplot(lpd_pca,ellipse=TRUE,groups=group)

# run PCA on all of the data
all<-merge(lpd,elements)%>%
  select(-date,-C,-Ni,-CN,-N15,-fvfm)%>%
  dplyr::rename(Microsite=Group)%>%
  dplyr::mutate(Microsite=ifelse(Microsite=="High Closed Canopy","High Position - Closed Canopy",ifelse(Microsite=="High Exposed","High Position - Open Canopy",ifelse(Microsite=="Low Closed Canopy","Low Position - Closed Canopy",""))))%>%
  dplyr::rename("\u03A8pd"=psipd)%>%
  dplyr::rename("\u03A8md"=psimd)%>%
  dplyr::rename("\u03C1chlorophyll"=chlcont)%>%
  dplyr::rename(LMA=lma)%>%
  dplyr::rename("Al:As"=huber)
all_pca<-prcomp(all[8:13],center=TRUE,scale.=TRUE)
all_summary<-as.data.frame(summary(all_pca)$importance) # 65% with 2
all_rotation<-as.data.frame(all_pca$rotation)
rownames(all_rotation) <- rownames(all_pca$rotation)


# obviously adding more variables isn't making things better
# let's look at the elemental analysis alone
elmnt_pca<-prcomp(elements[3:7],center=TRUE,scale.=TRUE)
elmnt_summary<-as.data.frame(summary(elmnt_pca)$importance) # 71% of the data with 2
group<-elements$Group
elmnt_plot<-ggbiplot(elmnt_pca,ellipse=TRUE,groups=group)


# it looks like the PCA on uncorrelated variables is best 
# the secrete lies with eigenvalues - highly correlated variables tend to have very small eigenvalues
# let's make a table and inspect it a little bit
all_rotation<-all_rotation%>%
  rownames_to_column()%>%
  dplyr::rename(Variable=rowname)

allpca_tab<-flextable(all_rotation)


# let's plot the results and see what it looks like

group<-all$Microsite
all_plot12<-ggbiplot(all_pca,choices=1:2,ellipse=TRUE,groups=group)


```
  
Clearly things are complicated with PCA. It seems to be most effective with a subset of the full dataset. This should make it clear that it isn't really a dimension reduction method. What's happening? This is called the __Curse of Dimensionality__.  
  
We have p variables, and that p can be large or small relative to n. As we progressively add variables and p increases, the dimension of the sample space is increasing. This effectively pulls observations apart as variance tends to increase (things get more spread out!). As this distance between observations increases, it becomes more difficult to summarize all of the variables in just a subset of the sample space. This is called the curse of dimensionality and is a huge problem in inference and prediction. We won't dive into this here, but this (SOURCE) resource explains things quite well.  
  
__Conclusion__  
Multivariate analysis can be an important tool in ecology. It has been used for many years (see the work of P.C. Eilou for early use of ordination methods in ecology), and will certainly be used for many more to come. As ecologists we often deal with very small sample sizes (field work is hard!) in high dimensional settings. This can be problematic and we shouldn't expect multivariate analysis to solve such problems as the curse of dimensionality. Prior to any statistical approach, it's the job of the researcher to carefully consider which variables are important. As we saw with the PCA data, this was clearly an issue and it extends to clustering, classification, and canonical correlation.  
While we only covered a few subjects here, mostly regarding statistical _inference_, many multivariate methods have been developed for prediction. Many of us use predictive models in some capacity to conduct our research. Even if we don't consider ourselves _modelers_, models are frequently used in the literature. Our hope is that the techniques we've introduced you to here will aid you in understanding, developing, testing, or even critiquing models in your field. With these tools you've been provided here, you should be ready to employ simple linear multivariate statistical methods on your own data, and even explore more complex approaches later.  



